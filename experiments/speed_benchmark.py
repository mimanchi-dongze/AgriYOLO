"""
Inference Speed (FPS/Latency) Benchmark Suite
--------------------------------------------------
Purpose:
    Measures the inference speed (Latency per image and FPS) for all SOTA 
    models using the same hardware environment. Essential for the 
    "Real-time Performance" section of the SCI paper.

Usage:
    python experiments/speed_benchmark.py
"""

import os
import sys
import time
import torch
import pandas as pd

# Ensure local 'ultralytics' is used (Must be before any ultralytics imports!)
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if root_path not in sys.path:
    sys.path.insert(0, root_path)

from ultralytics import YOLO, RTDETR, YOLOv10

def benchmark_speed():
    # ç»Ÿä¸€æµ‹è¯•å‚æ•°
    DATA_YAML = "agriyolo.yaml"
    IMG_SIZE = 640
    DEVICE = 0 if torch.cuda.is_available() else "cpu"
    WARMUP = 10  # é¢„çƒ­æ¬¡æ•°
    ITERATIONS = 50 # æµ‹è¯•æ¬¡æ•°
    
    # å¾…æµ‹è¯•æ¨¡å‹åˆ—è¡¨ (æŒ‡å‘ SOTA å¯¹æ¯”å®éªŒè®­ç»ƒå¥½çš„æœ€ä½³æƒé‡)
    # è·¯å¾„æ ¼å¼: SOTA_Comparisons/<ModelName>/weights/best.pt
    MODELS_TO_TEST = [
        {"name": "AgriYOLO",  "path": "SOTA_Comparisons/AgriYOLO/weights/best.pt",  "type": "v10"},
        {"name": "YOLOv10s",  "path": "SOTA_Comparisons/YOLOv10s/weights/best.pt",  "type": "v10"},
        {"name": "YOLOv8s",   "path": "SOTA_Comparisons/YOLOv8s/weights/best.pt",   "type": "v8"},
        {"name": "YOLOv9c",   "path": "SOTA_Comparisons/YOLOv9c/weights/best.pt",   "type": "v9"},
        {"name": "YOLOv5s",   "path": "SOTA_Comparisons/YOLOv5s/weights/best.pt",   "type": "v5"},
        {"name": "RT_DETR_l", "path": "SOTA_Comparisons/RT_DETR_l/weights/best.pt", "type": "rtdetr"}
    ]

    results = []

    print(f"ğŸš€ Starting speed benchmark on {DEVICE}...")

    for m in MODELS_TO_TEST:
        print(f"Testing {m['name']}...")
        
        if not os.path.exists(m['path']):
            print(f"âš ï¸ Warning: Weights not found at {m['path']}. Skipping...")
            continue

        # åŠ è½½æ¨¡å‹
        try:
            if m["type"] == "rtdetr": model = RTDETR(m["path"])
            elif m["type"] == "v10": model = YOLOv10(m["path"])
            else: model = YOLO(m["path"])
            
            model.to(DEVICE)
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)
            
            # 1. é¢„çƒ­
            for _ in range(WARMUP):
                _ = model(dummy_input, verbose=False)
            
            # 2. æ­£å¼æµ‹é€Ÿ
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            
            for _ in range(ITERATIONS):
                _ = model(dummy_input, verbose=False)
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            
            # è®¡ç®—å¹³å‡è€—æ—¶ (ms) å’Œ FPS
            avg_latency = ((end_time - start_time) / ITERATIONS) * 1000
            fps = 1000 / avg_latency
            
            results.append({
                "Model": m["name"],
                "Latency (ms)": round(avg_latency, 2),
                "FPS": round(fps, 1)
            })
            print(f"   Done: {avg_latency:.2f} ms | {fps:.1} FPS")
            
        except Exception as e:
            print(f"   Failed to test {m['name']}: {e}")

    # ä¿å­˜ç»“æœ
    if results:
        df = pd.DataFrame(results)
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        df.to_csv(os.path.join(log_dir, "speed_benchmark.csv"), index=False)
        print(f"\nâœ… Benchmark results saved to {log_dir}/speed_benchmark.csv")
        print(df.to_markdown(index=False))

if __name__ == "__main__":
    benchmark_speed()
